{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4cde8a0",
   "metadata": {},
   "source": [
    "GeekBrains, факультет Искусственного интелекта <br>\n",
    "курс \"Введение в обработку естественного языка\" (старт 21 июня 2023)\n",
    "\n",
    "### Урок 1. Обработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61acb86",
   "metadata": {},
   "source": [
    "Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3b1dd4",
   "metadata": {},
   "source": [
    "### Тема “Предобработка текста с помощью Python”\n",
    "\n",
    "Осуществим предобработку данных с Твиттера, чтобы очищенные данные в дальнейшем\n",
    "использовать для задачи классификации. Данный датасет содержит негативные (label = 1)\n",
    "и нейтральные (label = 0) высказывания. Для работы объединим train_df и test_df.\n",
    "\n",
    "Задания:\n",
    "\n",
    "1. Удалим @user из всех твитов с помощью паттерна \"@[\\w]*\". Для этого создадим\n",
    "функцию:\n",
    " ● для того, чтобы найти все вхождения паттерна в тексте, необходимо\n",
    "использовать re.findall(pattern, input_txt)\n",
    " ● для для замены @user на пробел, необходимо использовать re.sub()\n",
    " \n",
    "2. Изменим регистр твитов на нижний с помощью .lower().\n",
    "\n",
    "3. Заменим сокращения с апострофами (пример: ain't, can't) на пробел, используя\n",
    "apostrophe_dict. Для этого необходимо сделать функцию: для каждого слова в\n",
    "тексте проверить (for word in text.split()), если слово есть в словаре apostrophe_dict в\n",
    "качестве ключа (сокращенного слова), то заменить ключ на значение (полную\n",
    "версию слова).\n",
    "\n",
    "4. Заменим сокращения на их полные формы, используя short_word_dict. Для этого\n",
    "воспользуемся функцией, используемой в предыдущем пункте.\n",
    "\n",
    "5. Заменим эмотиконы (пример: \":)\" = \"happy\") на пробелы, используя emoticon_dict.\n",
    "Для этого воспользуемся функцией, используемой в предыдущем пункте.\n",
    "\n",
    "6. Заменим пунктуацию на пробелы, используя re.sub() и паттерн r'[^\\w\\s]'.\n",
    "\n",
    "7. Заменим спец. символы на пробелы, используя re.sub() и паттерн r'[^a-zA-Z0-9]'.\n",
    "\n",
    "8. Заменим числа на пробелы, используя re.sub() и паттерн r'[^a-zA-Z]'.\n",
    "\n",
    "9. Удалим из текста слова длиной в 1 символ, используя ' '.join([w for w in x.split() if\n",
    "len(w)>1]).\n",
    "\n",
    "10. Поделим твиты на токены с помощью nltk.tokenize.word_tokenize, создав новый\n",
    "столбец 'tweet_token'.\n",
    "\n",
    "11. Удалим стоп-слова из токенов, используя nltk.corpus.stopwords. Создадим столбец\n",
    "'tweet_token_filtered' без стоп-слов.\n",
    "\n",
    "12. Применим стемминг к токенам с помощью nltk.stem.PorterStemmer. Создадим\n",
    "столбец 'tweet_stemmed' после применения стемминга.\n",
    "\n",
    "13. Применим лемматизацию к токенам с помощью\n",
    "nltk.stem.wordnet.WordNetLemmatizer. Создадим столбец 'tweet_lemmatized' после\n",
    "применения лемматизации.\n",
    "\n",
    "14. Сохраним результат предобработки в pickle-файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b642ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\s2e\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\s2e\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\s2e\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\s2e\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\s2e\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\s2e\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\users\\s2e\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\s2e\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\s2e\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\s2e\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "956469b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk as nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2815d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "912a269c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\s2e\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\s2e\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\s2e\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\s2e\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8239d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = './data_hw1/train_tweets.csv'\n",
    "TEST_PATH = './data_hw1/test_tweets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0014836",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(TRAIN_PATH, index_col= 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a6b43ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                              tweet\n",
       "id                                                          \n",
       "1       0   @user when a father is dysfunctional and is s...\n",
       "2       0  @user @user thanks for #lyft credit i can't us...\n",
       "3       0                                bihday your majesty\n",
       "4       0  #model   i love u take with u all the time in ...\n",
       "5       0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dabcce8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked\n",
      "  thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked\n"
     ]
    }
   ],
   "source": [
    "print(train_data['tweet'].iloc[1])\n",
    "print(re.sub('@[\\w]*','', train_data['tweet'].iloc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c524384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_stop_words(x, stop_set):\n",
    "    x = [w for w in x if w not in stop_set]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99a2101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(x, stemmer):\n",
    "    x = [stemmer.stem(w) for w in x]\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c578198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatising(x, lemmatiser):\n",
    "    x = [lemmatiser.lemmatize(w) for w in x]\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4674e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_apos(y, apostrophe_dict):\n",
    "    y = y.split()\n",
    "    for n, word in enumerate(y):\n",
    "        if word in apostrophe_dict:\n",
    "            del y[n]\n",
    "            y = y[:n] + apostrophe_dict[word].split() + y[n:]\n",
    "    return \" \".join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1da8055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(y, my_dict):    \n",
    "    for n, word in enumerate(y):\n",
    "        if word in my_dict:\n",
    "            y[n] = my_dict[word]            \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb90c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, apostrophe_dict, short_word_dict, emoticon_dict):\n",
    "    x = re.sub('@[\\w]*',' ', x)\n",
    "    x = replace_apos(x, apostrophe_dict)\n",
    "    x = re.sub('[^\\w\\s]',' ', x)\n",
    "    x = re.sub('[^a-zA-Z0-9]',' ', x)\n",
    "    x = re.sub('[^a-zA-Z]',' ', x)\n",
    "    x = x.lower()\n",
    "    x = x.split()    \n",
    "    x = replace(x, short_word_dict)\n",
    "    x = replace(x, emoticon_dict)\n",
    "    x = ' '.join([w for w in x if len(w)>1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "151b820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_dict = {\n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I will\",\n",
    "\"i'll've\": \"I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "short_word_dict = {\n",
    "\"121\": \"one to one\",\n",
    "\"a/s/l\": \"age, sex, location\",\n",
    "\"adn\": \"any day now\",\n",
    "\"afaik\": \"as far as I know\",\n",
    "\"afk\": \"away from keyboard\",\n",
    "\"aight\": \"alright\",\n",
    "\"alol\": \"actually laughing out loud\",\n",
    "\"b4\": \"before\",\n",
    "\"b4n\": \"bye for now\",\n",
    "\"bak\": \"back at the keyboard\",\n",
    "\"bf\": \"boyfriend\",\n",
    "\"bff\": \"best friends forever\",\n",
    "\"bfn\": \"bye for now\",\n",
    "\"bg\": \"big grin\",\n",
    "\"bta\": \"but then again\",\n",
    "\"btw\": \"by the way\",\n",
    "\"cid\": \"crying in disgrace\",\n",
    "\"cnp\": \"continued in my next post\",\n",
    "\"cp\": \"chat post\",\n",
    "\"cu\": \"see you\",\n",
    "\"cul\": \"see you later\",\n",
    "\"cul8r\": \"see you later\",\n",
    "\"cya\": \"bye\",\n",
    "\"cyo\": \"see you online\",\n",
    "\"dbau\": \"doing business as usual\",\n",
    "\"fud\": \"fear, uncertainty, and doubt\",\n",
    "\"fwiw\": \"for what it's worth\",\n",
    "\"fyi\": \"for your information\",\n",
    "\"g\": \"grin\",\n",
    "\"g2g\": \"got to go\",\n",
    "\"ga\": \"go ahead\",\n",
    "\"gal\": \"get a life\",\n",
    "\"gf\": \"girlfriend\",\n",
    "\"gfn\": \"gone for now\",\n",
    "\"gmbo\": \"giggling my butt off\",\n",
    "\"gmta\": \"great minds think alike\",\n",
    "\"h8\": \"hate\",\n",
    "\"hagn\": \"have a good night\",\n",
    "\"hdop\": \"help delete online predators\",\n",
    "\"hhis\": \"hanging head in shame\",\n",
    "\"iac\": \"in any case\",\n",
    "\"ianal\": \"I am not a lawyer\",\n",
    "\"ic\": \"I see\",\n",
    "\"idk\": \"I don't know\",\n",
    "\"imao\": \"in my arrogant opinion\",\n",
    "\"imnsho\": \"in my not so humble opinion\",\n",
    "\"imo\": \"in my opinion\",\n",
    "\"iow\": \"in other words\",\n",
    "\"ipn\": \"I’m posting naked\",\n",
    "\"irl\": \"in real life\",\n",
    "\"jk\": \"just kidding\",\n",
    "\"l8r\": \"later\",\n",
    "\"ld\": \"later, dude\",\n",
    "\"ldr\": \"long distance relationship\",\n",
    "\"llta\": \"lots and lots of thunderous applause\",\n",
    "\"lmao\": \"laugh my ass off\",\n",
    "\"lmirl\": \"let's meet in real life\",\n",
    "\"lol\": \"laugh out loud\",\n",
    "\"ltr\": \"longterm relationship\",\n",
    "\"lulab\": \"love you like a brother\",\n",
    "\"lulas\": \"love you like a sister\",\n",
    "\"luv\": \"love\",\n",
    "\"m/f\": \"male or female\",\n",
    "\"m8\": \"mate\",\n",
    "\"milf\": \"mother I would like to fuck\",\n",
    "\"oll\": \"online love\",\n",
    "\"omg\": \"oh my god\",\n",
    "\"otoh\": \"on the other hand\",\n",
    "\"pir\": \"parent in room\",\n",
    "\"ppl\": \"people\",\n",
    "\"r\": \"are\",\n",
    "\"rofl\": \"roll on the floor laughing\",\n",
    "\"rpg\": \"role playing games\",\n",
    "\"ru\": \"are you\",\n",
    "\"shid\": \"slaps head in disgust\",\n",
    "\"somy\": \"sick of me yet\",\n",
    "\"sot\": \"short of time\",\n",
    "\"thanx\": \"thanks\",\n",
    "\"thx\": \"thanks\",\n",
    "\"ttyl\": \"talk to you later\",\n",
    "\"u\": \"you\",\n",
    "\"ur\": \"you are\",\n",
    "\"uw\": \"you’re welcome\",\n",
    "\"wb\": \"welcome back\",\n",
    "\"wfm\": \"works for me\",\n",
    "\"wibni\": \"wouldn't it be nice if\",\n",
    "\"wtf\": \"what the fuck\",\n",
    "\"wtg\": \"way to go\",\n",
    "\"wtgp\": \"want to go private\",\n",
    "\"ym\": \"young man\",\n",
    "\"gr8\": \"great\"\n",
    "}\n",
    "\n",
    "\n",
    "emoticon_dict = {\n",
    "\":)\": \"happy\",\n",
    "\":‑)\": \"happy\",\n",
    "\":-]\": \"happy\",\n",
    "\":-3\": \"happy\",\n",
    "\":->\": \"happy\",\n",
    "\"8-)\": \"happy\",\n",
    "\":-}\": \"happy\",\n",
    "\":o)\": \"happy\",\n",
    "\":c)\": \"happy\",\n",
    "\":^)\": \"happy\",\n",
    "\"=]\": \"happy\",\n",
    "\"=)\": \"happy\",\n",
    "\"<3\": \"happy\",\n",
    "\":-(\": \"sad\",\n",
    "\":(\": \"sad\",\n",
    "\":c\": \"sad\",\n",
    "\":<\": \"sad\",\n",
    "\":[\": \"sad\",\n",
    "\">:[\": \"sad\",\n",
    "\":{\": \"sad\",\n",
    "\">:(\": \"sad\",\n",
    "\":-c\": \"sad\",\n",
    "\":-< \": \"sad\",\n",
    "\":-[\": \"sad\",\n",
    "\":-||\": \"sad\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5fc29b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thanks for lyft credit cannot use cause do not don offer wheelchair vans in pdx disapointed getthanked'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(train_data['tweet'].iloc[1],  apostrophe_dict, short_word_dict, emoticon_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "302af205",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tweet'] = train_data['tweet'].apply(lambda x: preprocess(x,  apostrophe_dict, short_word_dict, emoticon_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d226ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tweet_token'] = train_data['tweet'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cba8617",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fce7e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tweet_token_filtered'] = train_data['tweet_token'].apply(lambda x: del_stop_words(x, stop_words) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ccda803",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8cbafd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tweet_stemmed'] = train_data['tweet_token_filtered'].apply(lambda x: stemming(x, stemmer) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10f08eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f0545c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tweet_lemmatized'] = train_data['tweet_token_filtered'].apply(lambda x: lemmatising(x, lemmatiser) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cc3e945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunctional, selfish, drag, kid, dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>thanks for lyft credit cannot use cause do not...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>model love you take with you all the time in y...</td>\n",
       "      <td>[model, love, you, take, with, you, all, the, ...</td>\n",
       "      <td>[model, love, take, time]</td>\n",
       "      <td>[model, love, take, time]</td>\n",
       "      <td>[model, love, take, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide society now motivation</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "      <td>[factsguid, societi, motiv]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>0</td>\n",
       "      <td>ate isz that youuu</td>\n",
       "      <td>[ate, isz, that, youuu]</td>\n",
       "      <td>[ate, isz, youuu]</td>\n",
       "      <td>[ate, isz, youuu]</td>\n",
       "      <td>[ate, isz, youuu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31959</th>\n",
       "      <td>0</td>\n",
       "      <td>to see nina turner on the airwaves trying to w...</td>\n",
       "      <td>[to, see, nina, turner, on, the, airwaves, try...</td>\n",
       "      <td>[see, nina, turner, airwaves, trying, wrap, ma...</td>\n",
       "      <td>[see, nina, turner, airwav, tri, wrap, mantl, ...</td>\n",
       "      <td>[see, nina, turner, airwave, trying, wrap, man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31960</th>\n",
       "      <td>0</td>\n",
       "      <td>listening to sad songs on monday morning otw t...</td>\n",
       "      <td>[listening, to, sad, songs, on, monday, mornin...</td>\n",
       "      <td>[listening, sad, songs, monday, morning, otw, ...</td>\n",
       "      <td>[listen, sad, song, monday, morn, otw, work, sad]</td>\n",
       "      <td>[listening, sad, song, monday, morning, otw, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31961</th>\n",
       "      <td>1</td>\n",
       "      <td>sikh temple vandalised in in calgary wso conde...</td>\n",
       "      <td>[sikh, temple, vandalised, in, in, calgary, ws...</td>\n",
       "      <td>[sikh, temple, vandalised, calgary, wso, conde...</td>\n",
       "      <td>[sikh, templ, vandalis, calgari, wso, condemn,...</td>\n",
       "      <td>[sikh, temple, vandalised, calgary, wso, conde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31962</th>\n",
       "      <td>0</td>\n",
       "      <td>thank you for you follow</td>\n",
       "      <td>[thank, you, for, you, follow]</td>\n",
       "      <td>[thank, follow]</td>\n",
       "      <td>[thank, follow]</td>\n",
       "      <td>[thank, follow]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31962 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                              tweet  \\\n",
       "id                                                                \n",
       "1          0  when father is dysfunctional and is so selfish...   \n",
       "2          0  thanks for lyft credit cannot use cause do not...   \n",
       "3          0                                bihday your majesty   \n",
       "4          0  model love you take with you all the time in y...   \n",
       "5          0                  factsguide society now motivation   \n",
       "...      ...                                                ...   \n",
       "31958      0                                 ate isz that youuu   \n",
       "31959      0  to see nina turner on the airwaves trying to w...   \n",
       "31960      0  listening to sad songs on monday morning otw t...   \n",
       "31961      1  sikh temple vandalised in in calgary wso conde...   \n",
       "31962      0                           thank you for you follow   \n",
       "\n",
       "                                             tweet_token  \\\n",
       "id                                                         \n",
       "1      [when, father, is, dysfunctional, and, is, so,...   \n",
       "2      [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "3                                [bihday, your, majesty]   \n",
       "4      [model, love, you, take, with, you, all, the, ...   \n",
       "5                 [factsguide, society, now, motivation]   \n",
       "...                                                  ...   \n",
       "31958                            [ate, isz, that, youuu]   \n",
       "31959  [to, see, nina, turner, on, the, airwaves, try...   \n",
       "31960  [listening, to, sad, songs, on, monday, mornin...   \n",
       "31961  [sikh, temple, vandalised, in, in, calgary, ws...   \n",
       "31962                     [thank, you, for, you, follow]   \n",
       "\n",
       "                                    tweet_token_filtered  \\\n",
       "id                                                         \n",
       "1      [father, dysfunctional, selfish, drags, kids, ...   \n",
       "2      [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "3                                      [bihday, majesty]   \n",
       "4                              [model, love, take, time]   \n",
       "5                      [factsguide, society, motivation]   \n",
       "...                                                  ...   \n",
       "31958                                  [ate, isz, youuu]   \n",
       "31959  [see, nina, turner, airwaves, trying, wrap, ma...   \n",
       "31960  [listening, sad, songs, monday, morning, otw, ...   \n",
       "31961  [sikh, temple, vandalised, calgary, wso, conde...   \n",
       "31962                                    [thank, follow]   \n",
       "\n",
       "                                           tweet_stemmed  \\\n",
       "id                                                         \n",
       "1      [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "2      [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "3                                      [bihday, majesti]   \n",
       "4                              [model, love, take, time]   \n",
       "5                            [factsguid, societi, motiv]   \n",
       "...                                                  ...   \n",
       "31958                                  [ate, isz, youuu]   \n",
       "31959  [see, nina, turner, airwav, tri, wrap, mantl, ...   \n",
       "31960  [listen, sad, song, monday, morn, otw, work, sad]   \n",
       "31961  [sikh, templ, vandalis, calgari, wso, condemn,...   \n",
       "31962                                    [thank, follow]   \n",
       "\n",
       "                                        tweet_lemmatized  \n",
       "id                                                        \n",
       "1      [father, dysfunctional, selfish, drag, kid, dy...  \n",
       "2      [thanks, lyft, credit, use, cause, offer, whee...  \n",
       "3                                      [bihday, majesty]  \n",
       "4                              [model, love, take, time]  \n",
       "5                      [factsguide, society, motivation]  \n",
       "...                                                  ...  \n",
       "31958                                  [ate, isz, youuu]  \n",
       "31959  [see, nina, turner, airwave, trying, wrap, man...  \n",
       "31960  [listening, sad, song, monday, morning, otw, w...  \n",
       "31961  [sikh, temple, vandalised, calgary, wso, conde...  \n",
       "31962                                    [thank, follow]  \n",
       "\n",
       "[31962 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "106f3154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14 Сохраним результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3343804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae648bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_hw1/preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4595f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
